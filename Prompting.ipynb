{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic\n",
      "Titanic dataset\n",
      "Titanic\n",
      "Titanic Dataset\n",
      "Titanic \n",
      "Titanic Dataset\n",
      "Titanic csv\n",
      "Titanic Dataset\n",
      "Titanic Data set\n",
      "Titanic\n",
      "Titanic dataset\n",
      "Titanic-Dataset (train.csv)\n",
      "Titanic Dataset\n",
      "Titanic extended dataset (Kaggle + Wikipedia)\n",
      "Titanic: cleaned data\n",
      "titanic\n",
      "titanic_dataset\n",
      "test titanic\n",
      "Titanic Survival Datasets\n",
      "Titanic\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "datasets = api.dataset_list(search='titanic')\n",
    "for dataset in datasets:\n",
    "    print(dataset.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"SOME API KEY\"\n",
    "genai.configure(api_key= GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash = genai.GenerativeModel(model_name= 'models/gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This example uses a single-turn, text-in/text-out structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Vector Database: Embeddings and Similarity Search\n",
       "\n",
       "A vector database is a specialized database designed to store and retrieve data represented as **numerical vectors**. These vectors, known as **embeddings**, capture the semantic meaning of the data, allowing for **similarity search**, a key feature of these databases.\n",
       "\n",
       "**Here's a breakdown:**\n",
       "\n",
       "* **Embeddings:** Instead of storing data in traditional formats like text or numbers, vector databases store data as high-dimensional vectors. These vectors are created by encoding the data using machine learning models (like word embeddings or image feature extractors) and represent the data's meaning and relationships. \n",
       "* **Similarity Search:**  Vector databases excel at finding data points similar to a given query vector. This is achieved by using **distance metrics** (like Euclidean distance or cosine similarity) to measure the closeness between vectors.\n",
       "* **Applications:** Vector databases are ideal for use cases involving:\n",
       "    * **Semantic search:** Finding similar documents, images, or other content based on meaning, not just keywords.\n",
       "    * **Recommendation systems:** Suggesting relevant products, movies, or articles based on user preferences.\n",
       "    * **Image and video analysis:** Searching for visually similar images or identifying objects in videos.\n",
       "    * **Fraud detection:** Identifying unusual patterns and anomalies in data.\n",
       "\n",
       "**How it Works:**\n",
       "\n",
       "1. **Data Embeddings:** Data is converted into numerical vectors using machine learning models.\n",
       "2. **Vector Storage:** The database stores these vectors in a way that allows efficient retrieval.\n",
       "3. **Querying:** A query is also converted into a vector.\n",
       "4. **Similarity Search:** The database finds the vectors most similar to the query vector using distance metrics.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "* **Semantic Understanding:**  Captures meaning and relationships between data points beyond literal keywords.\n",
       "* **Highly Scalable:**  Can handle large datasets and complex queries efficiently.\n",
       "* **Fast Retrieval:** Optimized for searching and finding similar data points quickly.\n",
       "\n",
       "**Examples of Vector Databases:**\n",
       "\n",
       "* **Pinecone:** A cloud-based vector database with a focus on scalability and ease of use.\n",
       "* **Weaviate:** A cloud-native vector database that combines vector search with graph database capabilities.\n",
       "* **Faiss:** A library developed by Facebook AI Research for efficient similarity search.\n",
       "* **Milvus:** An open-source vector database that offers high performance and scalability.\n",
       "\n",
       "**Overall, vector databases are transforming how we interact with information by enabling intelligent and efficient search based on semantic understanding. This technology is rapidly evolving and finding new applications in various fields.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = flash.generate_content(\"What is Vector Database?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## SCANN (Scalable Approximate Nearest Neighbors) Algorithm\n",
       "\n",
       "SCANN (Scalable Approximate Nearest Neighbors) is a powerful algorithm designed to efficiently search for nearest neighbors in massive datasets. It's particularly well-suited for high-dimensional data and offers a trade-off between accuracy and speed.\n",
       "\n",
       "**How it Works:**\n",
       "\n",
       "1. **Data Partitioning:** SCANN divides the data into a hierarchy of partitions, starting with a coarse-grained partitioning and progressively refining it. Each partition is associated with a representative point (centroid) in the data space.\n",
       "2. **Neighbor Search:** When searching for nearest neighbors of a query point, SCANN starts by finding the closest partition to the query. It then recursively searches the closest partitions within that partition until a desired level of refinement is reached.\n",
       "3. **Candidate Pool:** At each level of the hierarchy, SCANN maintains a candidate pool of potential nearest neighbors. These candidates are refined as the search progresses, and the final set of nearest neighbors is selected from the candidate pool.\n",
       "\n",
       "**Advantages:**\n",
       "\n",
       "* **Scalability:**  SCANN can efficiently handle massive datasets with billions of points due to its hierarchical partitioning approach and optimized search strategies.\n",
       "* **Accuracy:** By adjusting the search parameters, SCANN offers flexibility in balancing accuracy and speed. It can achieve near-exact results for small datasets or provide approximate solutions with significantly faster performance for large datasets.\n",
       "* **Efficiency:** SCANN is highly efficient in terms of computation time and memory usage, making it suitable for real-time applications.\n",
       "* **Versatility:** SCANN can be used for various applications, including:\n",
       "    * **Recommendation systems:** Finding similar items or users based on their features.\n",
       "    * **Image and video search:** Identifying similar images or videos based on their visual content.\n",
       "    * **Object detection and recognition:** Classifying objects based on their similarity to known objects.\n",
       "    * **Data clustering:** Grouping similar data points together.\n",
       "\n",
       "**Disadvantages:**\n",
       "\n",
       "* **Approximate Solutions:** SCANN provides approximate solutions, which may not always be accurate, especially for large datasets.\n",
       "* **Parameter Tuning:**  Choosing appropriate parameters for SCANN requires careful tuning to optimize its performance for specific datasets and applications.\n",
       "* **Memory Consumption:** While SCANN is generally memory-efficient, it may require significant memory for large datasets, especially when using higher refinement levels.\n",
       "* **Complex Implementation:** Implementing SCANN can be complex and require expertise in data structures, algorithms, and distributed systems.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "SCANN is a powerful and versatile algorithm for searching for nearest neighbors in large datasets. It offers a good balance between accuracy, scalability, and efficiency, making it suitable for various applications. However, it is important to understand its limitations and choose appropriate parameters to optimize its performance.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = flash.generate_content(\"what is SCANN (Scalable Approximate Nearest Neighbors) Algorithm? adbantages and disadvantages?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also set up a multi-turn chat structure too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Naruto Uzumaki is the titular protagonist of the popular manga and anime series \"Naruto\". He is a young ninja from the Hidden Leaf Village who dreams of becoming Hokage, the leader of his village. \n",
       "\n",
       "Here are some key points about Naruto:\n",
       "\n",
       "**Background:**\n",
       "\n",
       "* **Orphaned:** Naruto was orphaned at birth and grew up as an outcast, ostracized by the villagers for harboring the Nine-Tailed Fox, a powerful demon sealed inside him by the Fourth Hokage.\n",
       "* **Hyperactive and Mischievous:** Naruto is known for his loud and boisterous personality, often seeking attention and getting into trouble. He is determined and never gives up, even when facing insurmountable odds.\n",
       "* **Strong Will and Determination:** Despite his difficult upbringing, Naruto possesses an unwavering determination and resilience. He is always striving to improve himself and become a better ninja.\n",
       "\n",
       "**Powers and Abilities:**\n",
       "\n",
       "* **Jutsu:** Naruto is a skilled ninja with a wide range of jutsu, including the Rasengan, a powerful chakra-based attack.\n",
       "* **Nine-Tailed Fox:** Naruto's greatest strength lies in the Nine-Tailed Fox, which gives him incredible power and stamina.\n",
       "* **Sage Mode:** Naruto can enter Sage Mode, a state of heightened senses and abilities, granting him even greater power.\n",
       "\n",
       "**Character Development:**\n",
       "\n",
       "* **Growth and Maturity:** Throughout the series, Naruto undergoes significant character development, learning valuable lessons about friendship, responsibility, and the importance of hard work.\n",
       "* **Bonds with Others:** He forms strong bonds with his teammates, Sasuke Uchiha and Sakura Haruno, and with his fellow ninjas, including Kakashi Hatake and Jiraiya.\n",
       "* **Hokage Dreams:** His ultimate goal of becoming Hokage drives him to push his limits and become a true leader for his village.\n",
       "\n",
       "**Impact:**\n",
       "\n",
       "* **Popular and Beloved Character:** Naruto is one of the most popular and beloved anime characters of all time. His story resonates with audiences around the world, inspiring them with his determination and resilience.\n",
       "* **Cultural Influence:** The Naruto series has had a significant cultural impact, influencing fashion, music, and gaming.\n",
       "\n",
       "Overall, Naruto Uzumaki is a complex and multifaceted character whose journey inspires countless fans. He is a testament to the power of hard work, determination, and the bonds of friendship.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = flash.start_chat(history=[])\n",
    "response = chat.send_message('Hello! Can you tell me something about Naruto Uzumaki?')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "That's a great question!  Naruto doesn't actually *have* toad powers in the way that the toads themselves do. However, he has a very strong connection to the toads of Mount Myōboku, the legendary home of the toads in the Naruto universe. \n",
       "\n",
       "Here's how it works:\n",
       "\n",
       "* **Sage Mode:** Naruto learns Sage Mode from the toads. This allows him to absorb natural energy from the surroundings, significantly boosting his strength, speed, and senses.\n",
       "* **Toad Summoning Jutsu:** Naruto can summon toads, particularly Gamakichi and Gamabunta, to aid him in battle. These toads have their own unique abilities and powers, which Naruto can utilize.\n",
       "* **Toad Sage:** While Naruto doesn't become a toad himself, he is considered a \"Toad Sage\" because of his mastery of Sage Mode and his close bond with the toads. \n",
       "\n",
       "So, while Naruto doesn't have actual toad powers in the way that a toad would, he does have a powerful connection to the toads and benefits greatly from their teachings and abilities. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('Does he have toad powers?')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Naruto first uses Sage Mode in the anime during the **Pain Assault Arc**, which takes place in episodes **139 - 151** and **162 - 167** (depending on whether you are watching the original Naruto anime or the \"Naruto: Shippuden\" series). This arc is also known as the \"Pain Invasion Arc\".\n",
       "\n",
       "It's important to note that Naruto actually **starts learning Sage Mode** during the **Jiraiya Training Arc**, but he doesn't fully master it and use it in a battle until the Pain Assault arc. \n",
       "\n",
       "Let me know if you have any other questions about Naruto! 😊 \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('When was the first ti,e did Naruto use sage mode?')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore generation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_model = genai.GenerativeModel(model_name= 'gemini-1.5-flash', generation_config= genai.GenerationConfig(max_output_tokens= 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The SCANN (Scalable Approximate Nearest Neighbor) algorithm efficiently finds approximate nearest neighbors in high-dimensional data. It leverages a hierarchical tree structure to partition the data space, enabling fast search. SCANN uses hashing and pruning techniques to reduce the search space, making it highly scalable for large datasets. Its accuracy is tunable, balancing speed with precision. SCANN finds applications in areas like image retrieval, recommendation systems, and anomaly detection. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = short_model.generate_content(\"write 50 word essay about SCANN algorithm?\")\n",
    "Markdown(data= response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Temperature Effects in Softmax\n",
    "\n",
    "Softmax is a fundamental mechanism used to determine the probability distribution over words in language models. Temperature is a parameter that can significantly influence the output by modifying this distribution.\n",
    "\n",
    "### Without Temperature Adjustment\n",
    "\n",
    "Without adjusting the temperature, softmax tends to amplify differences between probabilities. Words with initially high probabilities become even more likely, while words with lower probabilities are further suppressed. This leads to a very \"peaky\" probability distribution, where the most likely words dominate.\n",
    "\n",
    "### Effects of High Temperature (t = 200)\n",
    "\n",
    "With a very high temperature like 200, the probabilities are divided by a large number, which results in much smaller exponents in the softmax calculation:\n",
    "\n",
    "- For example, `exp(x_i / 200)` will be very close to 1 for most values of `x_i`. This is because `exp(0) = 1`, and as the value inside the `exp` function decreases, it approaches 1.\n",
    "- As a result, the softmax probabilities are more evenly distributed, even for words that initially had low probabilities.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Extreme Temperatures**: Very high temperatures (like 200) can essentially \"flatten\" the probability distribution, making the model less likely to select words with the highest original probabilities. This can lead to more unpredictable and potentially unusual outputs.\n",
    "- **Temperature Control**: Temperature is powerful because it adjusts the \"peakiness\" of the probability distribution:\n",
    "  - **Lower Temperatures** create sharp peaks, strongly favoring high-probability words.\n",
    "  - **Higher Temperatures** flatten the distribution, giving more chances to words with lower probabilities.\n",
    "\n",
    "### Example of Temperature Adjustment\n",
    "\n",
    "- **Low Temperature (t = 0.5)**: Suppose \"Apple\" has a probability of 0.8, and \"Watermelon\" has a probability of 0.05.\n",
    "- **High Temperature (t = 200)**: With a high temperature, \"Apple\" might have a probability of 0.18, and \"Watermelon\" might have a probability of 0.15. The gap between their probabilities has narrowed significantly.\n",
    "\n",
    "### Word Selection Process\n",
    "\n",
    "Here’s how the word selection process works in real scenarios:\n",
    "\n",
    "1. **Contextual Probabilities**: Based on the conversation so far, the model calculates probabilities for each word in its vocabulary. These probabilities reflect how likely each word is to be the next word in the sequence, given the context.\n",
    "\n",
    "2. **Temperature Adjustment**: Temperature is applied to these probabilities. Lower temperatures amplify high probabilities, making those words more likely to be chosen. Higher temperatures flatten the probabilities, giving more chances to less likely words.\n",
    "\n",
    "3. **Softmax Normalization**: The adjusted probabilities are then normalized using the softmax function. This ensures that the probabilities for all words add up to 1.\n",
    "\n",
    "4. **Random Selection**: The model uses a random number generator (like rolling a weighted die) to select the next word based on the normalized probabilities. The higher the probability of a word, the more likely it is to be chosen.\n",
    "\n",
    "### Example of Word Selection\n",
    "\n",
    "Let’s say the model has a vocabulary of 10 words, and after considering the context and applying temperature, the normalized probabilities look like this:\n",
    "\n",
    "- Word 1: 0.35\n",
    "- Word 2: 0.15\n",
    "- Word 3: 0.05\n",
    "- Word 4: 0.20\n",
    "- Word 5: 0.05\n",
    "- Word 6: 0.10\n",
    "- Word 7: 0.05\n",
    "- Word 8: 0.05\n",
    "- Word 9: 0.00\n",
    "- Word 10: 0.00\n",
    "\n",
    "The model then uses a random number generator to select a word. It might choose Word 1 (most likely), but it could also choose Word 2, Word 4, or even Word 7, even though their probabilities are lower.\n",
    "\n",
    "### Randomness is Inherent\n",
    "\n",
    "Even with the most precise probability calculations, the final word selection still involves a random element. This randomness makes the model’s responses more diverse and prevents them from being overly predictable.\n",
    "\n",
    "### Practical Considerations in Real Scenarios\n",
    "\n",
    "- **Massive Vocabulary**: Large language models have vocabularies of millions of words. The probability calculations are complex, but the process remains essentially the same.\n",
    "- **Dynamic Context**: The model constantly adjusts its probabilities based on the ongoing conversation, which allows for a more natural and coherent flow of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retry Policy Explanation\n",
    "\n",
    "When working with APIs, a retry policy is often implemented to handle errors gracefully.\n",
    "\n",
    "- **`retry.Retry`**: This creates a retry policy object that tells the system how to handle retrying the request if an error occurs.\n",
    "- **`predicate=retry.if_transient_error`**: This specifies **when** to retry. In this case, it retries when the error is a **transient error**—for example, a temporary issue like quota limits or a network glitch.\n",
    "- **`initial=10`**: This is the **initial wait time** before retrying (in seconds).\n",
    "- **`multiplier=1.5`**: The **multiplier** is used to increase the wait time after each subsequent failure. For each retry, the wait time will be multiplied by 1.5, creating a backoff mechanism.\n",
    "- **`timeout=300`**: This is the **maximum amount of time** (in seconds) that the retry attempts should continue before giving up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      "\n",
      "Purple \n",
      "\n",
      "Blue \n",
      "\n",
      "Blue. \n",
      "\n",
      "Purple. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "# temperature: temperature must be in the range [0.0, 2.0].\n",
    "high_temp_model = genai.GenerativeModel(model_name= 'gemini-1.5-flash', generation_config= genai.GenerationConfig(temperature= 2.0))\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {'retry': retry.Retry(predicate= retry.if_transient_error, initial= 10, multiplier= 1.5, timeout= 300)}\n",
    "\n",
    "# Note that if you see a 429 Resource Exhausted error here, you may be able to edit the words in the prompt slightly to progress.\n",
    "for _ in range(5):\n",
    "    response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)', request_options= retry_policy)\n",
    "    if response.parts:\n",
    "        print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      "\n",
      "Purple \n",
      "\n",
      "Purple \n",
      "\n",
      "Purple \n",
      "\n",
      "Purple \n",
      "\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(model_name= 'gemini-1.5-flash', generation_config= genai.GenerationConfig(temperature= 0.0))\n",
    "retry_policy = {'retry': retry.Retry(predicate= retry.if_transient_error, initial= 10, multiplier= 1.5, timeout= 300)}\n",
    "\n",
    "for _ in range(5):\n",
    "    response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)', request_options= retry_policy)\n",
    "    if response.parts:\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K and Top-P in Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top-K** and **Top-P** are methods used in text generation to control the diversity of the model’s output. They decide how the next token in a sequence is selected from a range of possibilities.\n",
    "\n",
    "#### Top-K Sampling\n",
    "- **Top-K** is a **positive integer** that defines the number of most probable tokens from which to select the output token.\n",
    "- For example:\n",
    "  - Imagine the model predicts the next word and assigns probabilities to 10 possible words. If `Top-K` is set to **3**, only the **top 3 most probable words** are considered for selection.\n",
    "  - Let’s say the probabilities for the next token are:\n",
    "    - Word A: 0.40\n",
    "    - Word B: 0.30\n",
    "    - Word C: 0.20\n",
    "    - Word D: 0.05\n",
    "    - Word E: 0.05\n",
    "  - With `Top-K = 3`, only **Word A, B, and C** will be considered, and the remaining words will be ignored.\n",
    "- **Top-K = 1** results in **greedy decoding**, where the highest probability word is always chosen.\n",
    "\n",
    "#### Top-P (Nucleus Sampling)\n",
    "- **Top-P** (also known as **nucleus sampling**) is a **probability threshold**. It considers the smallest set of tokens whose cumulative probability exceeds a specified value.\n",
    "- For example:\n",
    "  - Let’s say the model assigns the following probabilities:\n",
    "    - Word A: 0.40\n",
    "    - Word B: 0.30\n",
    "    - Word C: 0.15\n",
    "    - Word D: 0.10\n",
    "    - Word E: 0.05\n",
    "  - If `Top-P` is set to **0.85**, the model will consider the **top tokens** until their cumulative probability exceeds **0.85**.\n",
    "  - In this case, **Words A, B, and C** will be selected, as their cumulative probability is **0.40 + 0.30 + 0.15 = 0.85**.\n",
    "- **Top-P = 0** is equivalent to **greedy decoding**, while **Top-P = 1** means **all tokens** are considered, leading to maximum diversity.\n",
    "\n",
    "### Combined Use of Top-K and Top-P\n",
    "- When both **Top-K** and **Top-P** are supplied, the model first filters the **top-K tokens**, then applies the **Top-P threshold** to select from the candidates.\n",
    "- This combined approach helps in controlling both the number of tokens and the cumulative probability, providing a fine balance between focusing on the most probable words and maintaining diversity.\n",
    "\n",
    "### Practical Example\n",
    "- Suppose you ask the model to complete a sentence.\n",
    "- **Top-K = 5** means the model considers only the top 5 words with the highest probabilities.\n",
    "- **Top-P = 0.9** means the model then selects from the top tokens until their cumulative probability reaches **90%**.\n",
    "- After these steps, the final token is chosen based on the supplied **temperature**, adding a degree of randomness.\n",
    "\n",
    "These techniques are essential to strike a balance between generating **creative, varied** outputs and ensuring the generated text remains **coherent and contextually relevant**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompts can be categorized in various ways depending on their purpose and structure. Here's a breakdown of common types:\n",
       "\n",
       "**Based on Purpose:**\n",
       "\n",
       "* **Informational Prompts:** Seek information or explanation.  Examples:\n",
       "    * \"What is the capital of France?\"\n",
       "    * \"Explain the process of photosynthesis.\"\n",
       "    * \"Summarize the plot of Hamlet.\"\n",
       "\n",
       "* **Instructional Prompts:** Request specific actions or steps. Examples:\n",
       "    * \"Write a poem about a summer day.\"\n",
       "    * \"Design a logo for a coffee shop.\"\n",
       "    * \"Solve the following equation.\"\n",
       "\n",
       "* **Creative Prompts:** Encourage imaginative and original thinking. Examples:\n",
       "    * \"Imagine you could fly. What would you do?\"\n",
       "    * \"Write a story about a talking dog.\"\n",
       "    * \"Create a painting that expresses joy.\"\n",
       "\n",
       "* **Comparative Prompts:** Ask for comparison and contrast between two or more things. Examples:\n",
       "    * \"Compare and contrast the French and American Revolutions.\"\n",
       "    * \"What are the similarities and differences between cats and dogs?\"\n",
       "\n",
       "* **Evaluative Prompts:** Require judgment or critical analysis. Examples:\n",
       "    * \"Evaluate the effectiveness of the government's new policy.\"\n",
       "    * \"Critique the author's use of symbolism in the novel.\"\n",
       "    * \"What are the advantages and disadvantages of using solar energy?\"\n",
       "\n",
       "* **Problem-Solving Prompts:** Present a problem and ask for a solution. Examples:\n",
       "    * \"How can we reduce traffic congestion in the city?\"\n",
       "    * \"Design a bridge that can withstand earthquakes.\"\n",
       "    * \"Troubleshoot this computer error.\"\n",
       "\n",
       "\n",
       "**Based on Structure:**\n",
       "\n",
       "* **Open-ended Prompts:** Allow for a wide range of responses and encourage exploration. Examples:\n",
       "    * \"What are your thoughts on climate change?\"\n",
       "    * \"Tell me a story.\"\n",
       "\n",
       "* **Closed-ended Prompts:**  Require specific, often short answers. Examples:\n",
       "    * \"What is 2 + 2?\"\n",
       "    * \"Is Paris the capital of France?\"\n",
       "\n",
       "* **Multiple-Choice Prompts:** Offer a limited set of answer options. Examples:\n",
       "    * \"Which of the following is a renewable energy source? (a) Coal (b) Oil (c) Solar (d) Natural gas\"\n",
       "\n",
       "* **Fill-in-the-Blank Prompts:** Require completing a sentence or phrase. Examples:\n",
       "    * \"The capital of France is ______.\"\n",
       "\n",
       "\n",
       "**Based on Modality:**\n",
       "\n",
       "* **Text-based Prompts:** Use written language. Examples: All the examples above.\n",
       "\n",
       "* **Image-based Prompts:** Use images as a stimulus. Examples:\n",
       "    * \"Describe what you see in this picture.\"\n",
       "    * \"Create a story based on this photograph.\"\n",
       "\n",
       "* **Audio-based Prompts:** Use sound as a stimulus. Examples:\n",
       "    * \"Identify the instrument playing in this recording.\"\n",
       "    * \"Write a song inspired by this piece of music.\"\n",
       "\n",
       "* **Video-based Prompts:** Use moving images and sound. Examples:\n",
       "    * \"Analyze the director's use of lighting in this scene.\"\n",
       "    * \"Create a sequel to this short film.\"\n",
       "\n",
       "* **Combined Modality Prompts:** Use a combination of text, images, audio, and/or video.\n",
       "\n",
       "\n",
       "This categorization is not mutually exclusive; a prompt can fall into multiple categories. For example, a prompt could be both informational and open-ended, or both instructional and text-based. Understanding these different types of prompts can help you craft effective prompts for various purposes, whether you're a teacher, writer, or AI developer.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name= 'models/gemini-1.5-pro', generation_config= genai.GenerationConfig(temperature= 1.0, top_k= 64, top_p= 0.95))\n",
    "\n",
    "prompt = 'What are the different types of prompts?'\n",
    "response = model.generate_content(prompt, request_options= retry_policy)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero-shot prompting** is a method where a model is given a prompt to perform a task **without any prior examples** or specific training for that particular task. The model must rely entirely on its pre-existing knowledge to generate an appropriate response. In zero-shot scenarios, the model understands the task directly from the way the prompt is phrased.\n",
    "\n",
    "#### Definition\n",
    "- **Zero-shot prompting** involves asking a model to complete a task or answer a question without providing any specific examples or additional context beforehand.\n",
    "- The term **\"zero-shot\"** comes from the fact that the model is given **zero examples** or demonstrations to help it perform the task.\n",
    "\n",
    "#### Example 1: Classification Task\n",
    "- **Prompt**: \"Is the following sentence positive or negative? 'The movie was incredibly boring and I wouldn't watch it again.'\"\n",
    "- In this case, the model is being asked to classify the sentiment of the sentence without any examples of what constitutes positive or negative sentiment. This is a **zero-shot** classification.\n",
    "\n",
    "#### Example 2: Generating an Answer\n",
    "- **Prompt**: \"Translate the following sentence to French: 'I am learning how to code.'\"\n",
    "- Here, the model is asked to **translate** the sentence directly, even though no prior translation examples are given.\n",
    "\n",
    "#### Example 3: General Question Answering\n",
    "- **Prompt**: \"What is the capital of Japan?\"\n",
    "- This is a **zero-shot** prompt because the model is directly being asked a factual question without being trained on how to answer such questions specifically in the current context.\n",
    "\n",
    "#### Summary\n",
    "Zero-shot prompting is useful when you want the model to leverage its extensive training knowledge and apply it to a new task without additional guidance. It allows for a wide range of flexible, on-the-fly tasks that can be completed with a well-phrased prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sentiment: **POSITIVE** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name= 'gemini-1.5-flash', generation_config= genai.GenerationConfig(temperature= 1.0, top_p= 1.0, max_output_tokens= 20))\n",
    "\n",
    "retry_policy = {'retry': retry.Retry(predicate= retry.if_transient_error, initial= 10, multiplier= 1.5, timeout= 300)}\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Naruto Shippuden is an emotional roller coaster filled with amazing character development, intense battles, and a powerful story of friendship and perseverance. It captures the essence of what it means to never give up and has some of the best anime moments ever.\"\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response= model.generate_content(zero_shot_prompt, request_options= retry_policy )\n",
    "\n",
    "# if response.parts:\n",
    "#     print(Markdown(response.text))\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enum Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Enum mode** is a feature in the Gemini API that allows you to constrain the model's output to a fixed set of values. This can be particularly useful in situations where you need a **specific, concise response**, like when classifying text or making decisions with a well-defined set of categories.\n",
    "\n",
    "### Why Use Enum Mode?\n",
    "- The models are trained to **generate text**, and sometimes they may produce **more text** than required.\n",
    "- For instance, when asked to classify the sentiment of a review, the model might output the **label** but also include extra words, like \"Sentiment: POSITIVE\" or add explanations afterward.\n",
    "- **Enum mode** helps ensure that the output is restricted to a specific set of possible values, reducing the chance of extra, unnecessary text being generated.\n",
    "\n",
    "### Example Use Case:\n",
    "Suppose you have a prompt that asks the model to classify a movie review as **POSITIVE**, **NEUTRAL**, or **NEGATIVE**. Without Enum mode, the model could generate:\n",
    "\n",
    "- \"Sentiment: POSITIVE. The review is highly enthusiastic.\"\n",
    "- \"The review appears to be POSITIVE.\"\n",
    "\n",
    "These outputs are **informative** but may contain **more text** than you need.\n",
    "\n",
    "### Using Enum Mode:\n",
    "With **Enum mode**, you can constrain the model's response to just one of the allowed values: **\"POSITIVE\"**, **\"NEUTRAL\"**, or **\"NEGATIVE\"**.\n",
    "\n",
    "**Prompt Example with Enum Mode**:\n",
    "- Prompt: \"Classify movie reviews as POSITIVE, NEUTRAL, or NEGATIVE.\"\n",
    "- Enum mode constrains the response so that the output is:\n",
    "  - **\"POSITIVE\"**\n",
    "  - **\"NEUTRAL\"**\n",
    "  - **\"NEGATIVE\"**\n",
    "\n",
    "This ensures that the model outputs **only** one of the specified values, making the response more **predictable** and **usable** in structured workflows.\n",
    "\n",
    "### Benefits:\n",
    "- **Consistency**: Enum mode guarantees that the output will always be one of the allowed values.\n",
    "- **Controlled Output**: It prevents the model from generating any additional or unnecessary text.\n",
    "- **Useful for Classification**: Enum mode is ideal for classification tasks where only a predefined set of responses is valid.\n",
    "\n",
    "By using **Enum mode**, you can make sure that the model stays focused on the expected output, especially in scenarios where the extra generated text can lead to ambiguity or make parsing the response more complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = 'Positive'\n",
    "    NEUTRAL = 'Neutral'\n",
    "    NEGATIVE = 'Negative'\n",
    "\n",
    "model = genai.GenerativeModel(model_name= 'gemini-1.5-flash-001', generation_config= genai.GenerationConfig(temperature= 0.1, response_mime_type= 'text/x.enum', response_schema= Sentiment))\n",
    "\n",
    "response = model.generate_content(contents= zero_shot_prompt, request_options= retry_policy)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot and Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**One-shot** and **few-shot** prompting are techniques used to improve the model's understanding of what is expected by providing examples of the desired response. These techniques help guide the model to produce more accurate or contextually relevant outputs.\n",
    "\n",
    "### One-Shot Prompting\n",
    "- **One-shot prompting** involves providing **one example** of the task to the model before asking it to perform the task itself.\n",
    "- This is useful when you want to **demonstrate** the format of the expected response or give the model some context to understand the type of output required.\n",
    "\n",
    "**Example of One-Shot Prompting**:\n",
    "- **Prompt**: \"Classify movie reviews as POSITIVE, NEUTRAL, or NEGATIVE. Here is an example:\n",
    "  - Review: 'The movie was quite engaging and the acting was fantastic.'\n",
    "  - Sentiment: POSITIVE\n",
    "\n",
    "  Now classify the following review:\n",
    "  - Review: 'The plot was confusing and hard to follow.'\n",
    "  - Sentiment:\"\n",
    "\n",
    "- **Explanation**: The prompt provides **one example** to show the model how the response should look, making it easier for the model to classify the second review correctly.\n",
    "\n",
    "### Few-Shot Prompting\n",
    "- **Few-shot prompting** involves providing **multiple examples** of how to perform the task before asking the model to do it.\n",
    "- This technique is helpful when the task is more complex, or you want to ensure the model fully understands the expected format and context.\n",
    "\n",
    "**Example of Few-Shot Prompting**:\n",
    "- **Prompt**: \"Classify movie reviews as POSITIVE, NEUTRAL, or NEGATIVE. Here are some examples:\n",
    "  - Review: 'The cinematography was stunning, and I loved the soundtrack.'\n",
    "  - Sentiment: POSITIVE\n",
    "  - Review: 'The movie was okay, but it wasn't very memorable.'\n",
    "  - Sentiment: NEUTRAL\n",
    "  - Review: 'The pacing was too slow, and the story was not interesting.'\n",
    "  - Sentiment: NEGATIVE\n",
    "\n",
    "  Now classify the following review:\n",
    "  - Review: 'The characters were well-developed, but the ending was disappointing.'\n",
    "  - Sentiment:\"\n",
    "\n",
    "- **Explanation**: By providing **multiple examples**, the model is given more context, which helps it make better decisions about how to classify the review.\n",
    "\n",
    "### Summary of Differences\n",
    "- **One-Shot Prompting**: Provides **one example** to help the model understand what is expected.\n",
    "- **Few-Shot Prompting**: Provides **multiple examples** to give the model more context and guidance.\n",
    "\n",
    "### Benefits of One-Shot and Few-Shot Prompting\n",
    "- **Improved Understanding**: Providing examples helps the model better understand the structure and type of response you are looking for.\n",
    "- **Increased Accuracy**: The model is more likely to produce an accurate response if it has context or examples to follow.\n",
    "- **Less Ambiguity**: Examples help reduce ambiguity, particularly for tasks where there could be multiple interpretations.\n",
    "\n",
    "By using **one-shot** or **few-shot** prompting, you can significantly improve the quality of the model’s output, especially for tasks where clarity and precision are important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "\"size\": \"large\",\n",
       "\"type\": \"normal\",\n",
       "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
       "}\n",
       "``` \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name= 'gemini-1.5-flash-latest', generation_config= genai.GenerationConfig(temperature= 0.1, top_p= 1.0, max_output_tokens= 250))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = 'Give me a large with cheese & pineapple'\n",
    "\n",
    "response = model.generate_content(contents= [few_shot_prompt, customer_order], request_options= retry_policy)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON mode** is a feature in the Gemini API that allows you to control the format of the model's response by constraining it to a specific **JSON schema**. This feature ensures that the output adheres strictly to the structure you need, without additional text, markdown, or unnecessary elements.\n",
    "\n",
    "### Why Use JSON Mode?\n",
    "- **Schema Control**: JSON mode allows you to define a **specific schema** that the model must follow. This is helpful when you need consistent and structured outputs.\n",
    "- **No Extraneous Text**: It guarantees that the response is strictly in JSON format, with no additional explanations, markdown elements, or free text.\n",
    "- **Reduced Post-Processing**: Consistent formatting means less effort to parse or clean the response, making integration with other systems seamless.\n",
    "\n",
    "### Example Use Case\n",
    "Suppose you want the model to extract information from a review and return it in a structured format, such as sentiment classification along with key topics mentioned.\n",
    "\n",
    "**Example Prompt with JSON Mode**:\n",
    "- **Prompt**: \"Extract the sentiment and key topics from the following review and return it in the specified JSON format.\n",
    "\n",
    "  Review: 'The food was delicious, but the service was a bit slow. The atmosphere was nice though.'\n",
    "\n",
    "  Schema:\n",
    "  ```json\n",
    "  {\n",
    "    \"sentiment\": \"POSITIVE/NEUTRAL/NEGATIVE\",\n",
    "    \"topics\": [\"list of topics\"]\n",
    "  }\n",
    "\n",
    "### Expected JSON Output:\n",
    "```json\n",
    "{\n",
    "  \"sentiment\": \"NEUTRAL\",\n",
    "  \"topics\": [\"food\", \"service\", \"atmosphere\"]\n",
    "}\n",
    "\n",
    "### Benefits of JSON Mode\n",
    "- **Consistency**: The output is always in the same structure, reducing variability.\n",
    "- **Machine-Friendly**: JSON is a widely used format that can be easily parsed and processed by machines.\n",
    "- **Ease of Use in Automation**: The structured output can be directly used in other tools or pipelines, making automation simpler and more reliable.\n",
    "\n",
    "### Practical Considerations\n",
    "- **Complex Schemas**: You can provide complex JSON schemas for the model to follow.\n",
    "- **Controlled Decoding**: JSON mode constrains token selection during decoding, ensuring that the model generates tokens that fit the supplied schema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Type Hinting and the `typing` Module\n",
    "\n",
    "#### What is Type Hinting?\n",
    "**Type hinting** is a feature in Python that allows developers to **specify the expected data types** of variables, function arguments, and return values. This improves code readability, helps catch errors earlier, and allows integrated development environments (IDEs) to provide better **autocomplete** and **error-checking**.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "def greet(name: str) -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "```\n",
    "- **Explanation**:\n",
    "  - The `name: str` indicates that the `name` argument should be a string (`str`).\n",
    "  - The `-> str` specifies that the function will **return** a string.\n",
    "- **Benefits**: This makes the code **easier to understand** and ensures type consistency, reducing the chances of bugs.\n",
    "\n",
    "#### The `typing` Module\n",
    "The **`typing` module** was introduced in Python to add support for type hints. It includes various tools that allow you to declare **complex types** for variables, function arguments, and return values.\n",
    "\n",
    "**Key Features of the `typing` Module**:\n",
    "\n",
    "1. **Basic Type Hints**:\n",
    "   - You can use types like `int`, `str`, `list`, etc., for simple type hinting.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     def add_numbers(a: int, b: int) -> int:\n",
    "         return a + b\n",
    "     ```\n",
    "\n",
    "2. **`List`, `Dict`, `Tuple` for Collections**:\n",
    "   - The `typing` module provides ways to declare **types for collections** like lists and dictionaries.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from typing import List, Dict\n",
    "\n",
    "     def process_items(items: List[int]) -> Dict[str, int]:\n",
    "         return {\"sum\": sum(items)}\n",
    "     ```\n",
    "     - **Explanation**: `List[int]` means the function expects a list of integers, and `Dict[str, int]` means the return value will be a dictionary with `str` keys and `int` values.\n",
    "\n",
    "3. **`Optional` for Optional Arguments**:\n",
    "   - If an argument can be **`None`**, you can use `Optional`.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from typing import Optional\n",
    "\n",
    "     def greet(name: Optional[str] = None) -> str:\n",
    "         if name:\n",
    "             return f\"Hello, {name}!\"\n",
    "         return \"Hello!\"\n",
    "     ```\n",
    "\n",
    "### Why Use `typing_extensions`?\n",
    "\n",
    "The **`typing_extensions`** module is used when certain advanced typing features are **not yet available** in your version of the Python `typing` module. As Python evolves, new typing features are added, but they may not be immediately included in older Python versions. `typing_extensions` allows you to use **new and experimental features** even if your Python version doesn’t support them yet.\n",
    "\n",
    "#### Features of `typing_extensions` (Advanced Typing Features)\n",
    "Below, I explain some of the advanced features provided by `typing_extensions` that aren't found in older versions of the standard `typing` module.\n",
    "\n",
    "1. **`TypedDict`**\n",
    "   - **`TypedDict`** allows you to define a dictionary where the **keys have specific types**.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from typing_extensions import TypedDict\n",
    "\n",
    "     class PizzaOrder(TypedDict):\n",
    "         size: str\n",
    "         ingredients: list[str]\n",
    "         type: str\n",
    "\n",
    "     order: PizzaOrder = {\n",
    "         \"size\": \"large\",\n",
    "         \"ingredients\": [\"cheese\", \"tomato\"],\n",
    "         \"type\": \"vegetarian\"\n",
    "     }\n",
    "     ```\n",
    "   - **Explanation**: `TypedDict` helps ensure that the dictionary matches a particular structure, making it easier to understand and less error-prone.\n",
    "\n",
    "2. **`Literal`**\n",
    "   - **`Literal`** allows you to specify that a value must be **one of a set of specific values**.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from typing_extensions import Literal\n",
    "\n",
    "     def get_pizza_size(size: Literal[\"small\", \"medium\", \"large\"]) -> str:\n",
    "         return f\"You selected a {size} pizza.\"\n",
    "\n",
    "     print(get_pizza_size(\"medium\"))  # Output: You selected a medium pizza.\n",
    "     ```\n",
    "   - **Explanation**: `Literal` restricts the `size` argument to specific values (`\"small\"`, `\"medium\"`, `\"large\"`), ensuring type safety and preventing errors caused by incorrect values.\n",
    "\n",
    "3. **`Protocol`**\n",
    "   - **`Protocol`** is similar to an **interface** in other programming languages, defining a set of methods or attributes a class must implement.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from typing_extensions import Protocol\n",
    "\n",
    "     class Flyable(Protocol):\n",
    "         def fly(self) -> None:\n",
    "             ...\n",
    "\n",
    "     class Bird:\n",
    "         def fly(self) -> None:\n",
    "             print(\"Bird is flying.\")\n",
    "\n",
    "     def make_it_fly(obj: Flyable) -> None:\n",
    "         obj.fly()\n",
    "\n",
    "     bird = Bird()\n",
    "     make_it_fly(bird)  # Output: Bird is flying.\n",
    "     ```\n",
    "   - **Explanation**: `Protocol` defines an expected behavior that classes must adhere to, making sure that the class implements the necessary methods.\n",
    "\n",
    "4. **`Final`**\n",
    "   - **`Final`** is used to **prevent reassignment** or inheritance, ensuring that variables or classes cannot be modified or extended.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from typing_extensions import Final\n",
    "\n",
    "     MAX_SIZE: Final = 100\n",
    "\n",
    "     # MAX_SIZE = 200  # Type checker warning: MAX_SIZE should not be reassigned.\n",
    "     ```\n",
    "   - **Explanation**: `Final` helps ensure that certain values or methods remain unchanged, which helps maintain stability in the code.\n",
    "\n",
    "5. **`Self` Type**\n",
    "   - The `Self` type is used to indicate that a method returns an instance of the **current class**, enabling **method chaining**.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from typing_extensions import Self\n",
    "\n",
    "     class PizzaBuilder:\n",
    "         def add_cheese(self) -> Self:\n",
    "             print(\"Adding cheese.\")\n",
    "             return self\n",
    "\n",
    "         def add_tomato(self) -> Self:\n",
    "             print(\"Adding tomato.\")\n",
    "             return self\n",
    "\n",
    "     builder = PizzaBuilder()\n",
    "     builder.add_cheese().add_tomato()  # Output: Adding cheese. Adding tomato.\n",
    "     ```\n",
    "   - **Explanation**: `Self` is used in class methods to indicate that they return an instance of the class, supporting chaining methods together.\n",
    "\n",
    "### Summary of the Cohesive Explanation\n",
    "- **Type Hinting**: Adding type information to variables and functions to improve code readability and catch errors early.\n",
    "- **`typing` Module**: Part of Python’s standard library, used to implement basic type hinting (e.g., `List`, `Dict`, `Optional`).\n",
    "- **`typing_extensions`**: A separate module that provides **advanced typing features** not available in the standard `typing` module, useful for ensuring compatibility across Python versions.\n",
    "\n",
    "**Advanced Features in `typing_extensions`**:\n",
    "- **`TypedDict`**: Defines dictionaries with specific keys and value types.\n",
    "- **`Literal`**: Restricts values to a fixed set of options.\n",
    "- **`Protocol`**: Defines an expected structure (similar to an interface).\n",
    "- **`Final`**: Prevents reassignment or inheritance.\n",
    "- **`Self`**: Indicates that a method returns an instance of the current class.\n",
    "\n",
    "By understanding **type hinting** and these modules, you can write more **robust, readable, and maintainable** Python code. The **`typing_extensions`** module extends the capabilities of the `typing` module, making it possible to use cutting-edge features even in older Python versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Thought (CoT)\n",
    "\n",
    "**Chain of Thought (CoT) prompting** is a way to help a language model think more like a human by breaking down a problem into **smaller reasoning steps** before giving the final answer. Instead of directly answering a question, the model explains its thought process step by step, which usually leads to better and more accurate answers.\n",
    "\n",
    "#### Why Use Chain of Thought?\n",
    "- When you ask a model a question directly, it might give an answer that **sounds correct** but is actually **wrong**. This is called a **hallucination**.\n",
    "- By asking the model to show its reasoning (using CoT prompting), you can see the steps it takes to reach an answer, making it easier to spot mistakes.\n",
    "\n",
    "#### Example\n",
    "**Direct Prompting (No CoT)**:\n",
    "- Prompt: \"What is 13 times 12?\"\n",
    "- Response: \"156\"\n",
    "  - This answer is **fast** and uses fewer tokens, but there is a risk of it being incorrect if the model is uncertain.\n",
    "\n",
    "**Chain of Thought Prompting**:\n",
    "- Prompt: \"Calculate 13 times 12 step by step.\"\n",
    "- Response: \"First, break it down as 13 times 10, which is 130. Then add 13 times 2, which is 26. Adding 130 and 26 gives 156.\"\n",
    "  - **Explanation**: By showing the intermediate steps, the model is more likely to provide the correct answer, and you can also verify each part of the calculation.\n",
    "\n",
    "#### Pros and Cons\n",
    "- **Better Reasoning**: CoT prompting often leads to better accuracy because the model takes its time to reason through the problem.\n",
    "- **Higher Cost**: Since the model needs to output more steps, CoT prompting can be **more expensive** due to increased token usage.\n",
    "\n",
    "#### Real-Life Scenario\n",
    "If you ask the model, \"Is it safe to go out in a thunderstorm?\"\n",
    "- With **direct prompting**, the model might just say \"No, it's not safe.\" This answer is correct but lacks detail.\n",
    "- With **CoT prompting**, you can ask: \"Explain why it is or isn't safe to go out in a thunderstorm.\"\n",
    "  - Response: \"It's not safe to go out in a thunderstorm because of the risk of lightning strikes. Lightning can strike tall objects, and being outdoors makes you more vulnerable. It is safer to stay indoors until the storm passes.\"\n",
    "  - **Explanation**: The model gives reasoning steps that make the answer more informative and easy to understand.\n",
    "\n",
    "**Models like the Gemini family** are designed to be conversational and explain their reasoning, which makes them well-suited for CoT prompting. However, if you need a **shorter, direct answer**, you can instruct the model to be more concise in the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same approach, but indicate to the model that it should \"think step by step\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve this:\n",
       "\n",
       "* **When you were 4:** Your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
       "* **Age difference:**  The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "* **Current age:** Since you are now 20, your partner is 20 + 8 = **28 years old**. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct: Reason and Act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**ReAct** (Reason and Act) is a prompting technique that encourages the model to **reason through a problem** and then **take action** based on that reasoning. This approach helps break down complex tasks into smaller, manageable steps and allows the model to use both reasoning and actions to solve a problem effectively.\n",
    "\n",
    "In this example, you will use a ReAct prompt directly in the Gemini API and perform the **searching steps manually**. The ReAct prompt provides a structure where the model reasons about the question, identifies what information is needed, and then takes steps to obtain it. \n",
    "\n",
    "### How ReAct Works\n",
    "- **Reasoning Step**: The model first reasons about the problem. This might involve breaking down the question, identifying what information is needed, or understanding the sequence of actions required.\n",
    "- **Action Step**: The model takes an action based on its reasoning. This could involve a search, calculation, or making a specific decision.\n",
    "\n",
    "### Example\n",
    "**ReAct Prompt**:\n",
    "- Prompt: \"What is the population of France in 2023? Break down the steps you need to get the answer.\"\n",
    "- **Reasoning**: \"To answer this question, I need the most recent population data for France. I will need to look up reliable sources for current statistics.\"\n",
    "- **Action**: \"Perform a web search for 'France population 2023'.\"\n",
    "- **Final Answer**: The model will provide the final answer based on the results of the action.\n",
    "\n",
    "This approach is similar to how humans solve complex problems by **thinking first** and then **acting** to gather more information if needed.\n",
    "\n",
    "### Frameworks to Simplify ReAct Prompts\n",
    "ReAct prompts follow a well-defined structure, and there are frameworks like **LangChain** that wrap these prompts into easier-to-use APIs. These frameworks automate the process of reasoning and acting by making **tool calls** directly, saving you time and simplifying complex workflows.\n",
    "\n",
    "### Pros and Cons\n",
    "- **Improved Accuracy**: By breaking down tasks into reasoning and actions, ReAct helps models make more informed decisions.\n",
    "- **Manual Intervention**: In this example, you need to perform the searching steps yourself, which can be more time-consuming compared to direct answers.\n",
    "- **Framework Support**: Using frameworks like LangChain can make it easier to use ReAct prompting by automating tool calls.\n",
    "\n",
    "**Models like the Gemini family** are capable of reasoning and acting based on the prompts provided, making them suitable for ReAct techniques. However, these steps can take more tokens and time, so it’s important to balance between the level of reasoning required and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Who trained Naruto Uzumaki to control the Nine-Tails chakra?\n",
    "\n",
    "Thought 1\n",
    "The question is asking about the person who trained Naruto Uzumaki to control the Nine-Tails chakra. I need to search for 'Naruto Nine-Tails chakra training'.\n",
    "\n",
    "Action 1\n",
    "<search>Naruto Nine-Tails chakra training</search>\n",
    "\n",
    "Observation 1\n",
    "Naruto Uzumaki learned to control the Nine-Tails chakra under the guidance of Killer B, who was also a jinchūriki of the Eight-Tails.\n",
    "\n",
    "Thought 2\n",
    "The answer is Killer B because he helped Naruto learn to control the Nine-Tails chakra.\n",
    "\n",
    "Action 2\n",
    "<finish>Killer B</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "Who were the members of Team 7 in Naruto?\n",
    "\n",
    "Thought 1\n",
    "The question is asking for the members of Team 7 in Naruto. I will search for 'Team 7 members Naruto'.\n",
    "\n",
    "Action 1\n",
    "<search>Team 7 members Naruto</search>\n",
    "\n",
    "Observation 1\n",
    "Team 7 initially consisted of Naruto Uzumaki, Sasuke Uchiha, Sakura Haruno, and their sensei, Kakashi Hatake. Later, Sai joined the team after Sasuke's departure.\n",
    "\n",
    "Thought 2\n",
    "The members of Team 7 are Naruto Uzumaki, Sasuke Uchiha, Sakura Haruno, and Kakashi Hatake as the sensei. Later, Sai also became a member.\n",
    "\n",
    "Action 2\n",
    "<finish>Naruto Uzumaki, Sasuke Uchiha, Sakura Haruno, Kakashi Hatake, Sai</finish>\n",
    "\"\"\"\n",
    "\n",
    "example3 = \"\"\"Question\n",
    "What is the Rasengan, and who created it?\n",
    "\n",
    "Thought 1\n",
    "The question is asking about what the Rasengan is and who created it. I will search for 'Rasengan creation'.\n",
    "\n",
    "Action 1\n",
    "<search>Rasengan creation</search>\n",
    "\n",
    "Observation 1\n",
    "The Rasengan is a powerful spinning chakra ball technique created by Minato Namikaze, also known as the Fourth Hokage. It is a high-level technique that requires precise chakra control.\n",
    "\n",
    "Thought 2\n",
    "The Rasengan is a spinning chakra ball created by Minato Namikaze, the Fourth Hokage.\n",
    "\n",
    "Action 2\n",
    "<finish>Rasengan, created by Minato Namikaze</finish>\n",
    "\"\"\"\n",
    "\n",
    "example4 = \"\"\"Question\n",
    "What was the reason behind Sasuke Uchiha leaving Konoha?\n",
    "\n",
    "Thought 1\n",
    "The question is asking why Sasuke Uchiha left Konoha. I will search for 'Sasuke Uchiha leaves Konoha reason'.\n",
    "\n",
    "Action 1\n",
    "<search>Sasuke Uchiha leaves Konoha reason</search>\n",
    "\n",
    "Observation 1\n",
    "Sasuke Uchiha left Konoha to gain power from Orochimaru. He wanted to become strong enough to defeat his brother, Itachi Uchiha, and avenge the Uchiha clan.\n",
    "\n",
    "Thought 2\n",
    "Sasuke left Konoha to gain power from Orochimaru so he could defeat Itachi Uchiha.\n",
    "\n",
    "Action 2\n",
    "<finish>To gain power from Orochimaru to defeat Itachi Uchiha</finish>\n",
    "\"\"\"\n",
    "\n",
    "example5 = \"\"\"Question\n",
    "Who was the Fourth Hokage, and what was his relation to Naruto Uzumaki?\n",
    "\n",
    "Thought 1\n",
    "The question asks about the identity of the Fourth Hokage and his relation to Naruto Uzumaki. I need to search for 'Fourth Hokage Naruto relation'.\n",
    "\n",
    "Action 1\n",
    "<search>Fourth Hokage Naruto relation</search>\n",
    "\n",
    "Observation 1\n",
    "The Fourth Hokage was Minato Namikaze, and he was Naruto Uzumaki's father. Minato was known for his incredible speed and was also called the Yellow Flash of Konoha.\n",
    "\n",
    "Thought 2\n",
    "The Fourth Hokage was Minato Namikaze, and he was Naruto Uzumaki's father.\n",
    "\n",
    "Action 2\n",
    "<finish>Minato Namikaze, Naruto's father</finish>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To focus on capturing one step at a time and ignore any incorrect Observation steps, you can use stop_sequences to stop the generation process. The steps should follow the order: **Thought, Action, Observation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "The question is asking about the nature of Obito and Itachi's character. I should find out if they are considered good or bad in the Naruto story.\n",
      "\n",
      "Action 1\n",
      "<search>Obito and Itachi Naruto</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Are Obito and Itach bad guys?\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "    [model_instructions, example1, example2, question],\n",
    "    generation_config=config,\n",
    "    request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Thought 2\n",
       "Based on the observation, Obito started a major war and Itachi killed his entire clan. This suggests they both committed acts that are usually considered villainous.\n",
       "\n",
       "Action 2\n",
       "<finish>Yes, both Obito and Itachi are considered villains in the Naruto story.</finish> \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "Obito actually started the 4th great ninja war because of his love for Rin, and Itachi has killed his entire uchiha clan to save the village of the leaf\n",
    "\"\"\"\n",
    "\n",
    "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
    "Markdown(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process repeats until the <finish> action is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini family of models can be used to generate **code, configuration, and scripts**. This is useful in many scenarios, such as:\n",
    "- **Learning to Code**: Beginners can use code generation to understand programming concepts by seeing examples.\n",
    "- **Exploring a New Language**: Developers learning a new language can generate code snippets to see how specific tasks are performed.\n",
    "- **Rapid Prototyping**: When you need a quick initial version of a function or script, code generation can save you time.\n",
    "\n",
    "It's important to be aware that **LLMs (Large Language Models) can't reason perfectly** and can **repeat training data**, so it's essential to:\n",
    "1. **Read and test the generated code** to ensure it works as expected.\n",
    "2. **Comply with relevant licenses** if the generated code is used in a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import numpy as np\n",
       "\n",
       "def backpropagation(inputs, targets, weights1, weights2, learning_rate):\n",
       "  \"\"\"\n",
       "  Performs backpropagation for a simple neural network with one hidden layer.\n",
       "\n",
       "  Args:\n",
       "    inputs: Input data (n_samples, n_features)\n",
       "    targets: Target values (n_samples, n_outputs)\n",
       "    weights1: Weights of the first layer (n_features, n_hidden)\n",
       "    weights2: Weights of the second layer (n_hidden, n_outputs)\n",
       "    learning_rate: Learning rate for gradient descent\n",
       "\n",
       "  Returns:\n",
       "    Updated weights1, weights2\n",
       "  \"\"\"\n",
       "\n",
       "  # Forward pass\n",
       "  hidden_output = np.tanh(np.dot(inputs, weights1))\n",
       "  outputs = np.dot(hidden_output, weights2)\n",
       "\n",
       "  # Error calculation\n",
       "  error = targets - outputs\n",
       "\n",
       "  # Backpropagation\n",
       "  dweights2 = np.dot(hidden_output.T, error)\n",
       "  dweights1 = np.dot(inputs.T, np.dot(error, weights2.T) * (1 - hidden_output**2))\n",
       "\n",
       "  # Gradient descent update\n",
       "  weights1 += learning_rate * dweights1\n",
       "  weights2 += learning_rate * dweights2\n",
       "\n",
       "  return weights1, weights2\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Generating a Python function to calculate the factorial of a number.\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ))\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to perform neural network back propogation. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)\n",
    "\n",
    "# In this example, the model generates a **recursive function** to calculate the factorial of a number. \n",
    "# The `temperature` parameter is set to `1` to allow for creative responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Gemini API** can automatically **run generated code** and return the output, making it easier to test the code directly after generation. This feature is useful for validating the correctness of generated scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def sum_of_natural_numbers(n):\n",
       "  \"\"\"Calculates the sum of the first n natural numbers.\n",
       "\n",
       "  Args:\n",
       "    n: An integer representing the number of natural numbers to sum.\n",
       "\n",
       "  Returns:\n",
       "    The sum of the first n natural numbers.\n",
       "  \"\"\"\n",
       "  return n * (n + 1) // 2\n",
       "\n",
       "n = int(input(\"Enter a positive integer: \"))\n",
       "sum_of_numbers = sum_of_natural_numbers(n)\n",
       "print(f\"The sum of the first {n} natural numbers is: {sum_of_numbers}\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Calculating the sum of the first 14 prime numbers (only considering the odd primes).\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution',)\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Write a python program to calculate the sum of the n natural numbers.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated code calculates the sum of the n natural numbers. The **code_execution** tool allows the model to run the code and return the result.\n",
    "\n",
    "While this may look like a simple response, you can **inspect the response** to see each step: initial text, code generation, execution results, and final summary. This can be done using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"```python\\ndef sum_of_natural_numbers(n):\\n  \\\"\\\"\\\"Calculates the sum of the first n natural numbers.\\n\\n  Args:\\n    n: An integer representing the number of natural numbers to sum.\\n\\n  Returns:\\n    The sum of the first n natural numbers.\\n  \\\"\\\"\\\"\\n  return n * (n + 1) // 2\\n\\nn = int(input(\\\"Enter a positive integer: \\\"))\\nsum_of_numbers = sum_of_natural_numbers(n)\\nprint(f\\\"The sum of the first {n} natural numbers is: {sum_of_numbers}\\\")\\n```\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")\n",
    "\n",
    "# This approach helps in understanding the flow of execution and allows for better debugging of generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini family of models can also **explain code**, which is particularly helpful for:\n",
    "- **Beginners trying to understand unfamiliar code**.\n",
    "- **Developers reviewing existing code** to gain a high-level understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a Python script named `train.py` which trains a transformer model for machine translation.  Here's a high-level breakdown:\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "* **Imports Libraries:**  The script begins by importing necessary libraries including:\n",
       "    * `model.py`: Contains the definition of the transformer model architecture.\n",
       "    * `dataset.py`: Handles data loading, processing, and creating datasets for training and validation.\n",
       "    * `config.py`:  Holds configuration parameters (learning rate, batch size, etc.).\n",
       "    * `torchtext.datasets`: Provides access to common language datasets.\n",
       "    * `torch`, `torch.nn`, `torch.utils.data`:  PyTorch components for building and training neural networks.\n",
       "    * `tokenizers`: Used for tokenizing text into words or subwords.\n",
       "    * `torchmetrics`: Provides various metrics for evaluating translation performance.\n",
       "    * `torch.utils.tensorboard`: Used for logging training progress and visualizing results.\n",
       "* **Defines Functions:**\n",
       "    * `greedy_decode`:  Performs greedy decoding, a common approach for generating translations from a transformer model.\n",
       "    * `run_validation`:  Evaluates the model on a validation dataset, calculating metrics like character error rate (CER), word error rate (WER), and BLEU score.\n",
       "    * `get_all_sentences`:  Iterates through a dataset and yields all the sentences in a specified language.\n",
       "    * `get_or_build_tokenizer`:  Loads an existing tokenizer or trains a new one based on the provided dataset and language.\n",
       "    * `get_ds`:  Prepares the training and validation datasets, including tokenization and dataloading.\n",
       "    * `get_model`:  Builds the transformer model with specified vocabulary sizes and configuration parameters.\n",
       "    * `train_model`:  The core training function. It:\n",
       "        * Sets up the training environment (device selection, weights folder creation).\n",
       "        * Loads or trains tokenizers.\n",
       "        * Builds the model.\n",
       "        * Initializes an optimizer (Adam) and a loss function (CrossEntropyLoss with label smoothing).\n",
       "        * Loads pre-trained weights if specified.\n",
       "        * Iterates through epochs, training the model on the training dataset and evaluating it on the validation dataset.\n",
       "        * Saves the model weights at the end of each epoch.\n",
       "* **Main Execution:** \n",
       "    * The script uses a `if __name__ == '__main__'` block to ensure the code is only run when the script is executed directly (not imported as a module).\n",
       "    * It retrieves configuration settings from `config.py` and calls `train_model` to start the training process.\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "You would use this script to train a transformer model for machine translation. This script provides a structured framework for:\n",
       "\n",
       "* **Data Preparation:** It handles the necessary data processing, tokenization, and dataloading steps.\n",
       "* **Model Training:** It defines the model architecture, optimizes its weights using Adam, and uses a cross-entropy loss function with label smoothing.\n",
       "* **Evaluation:** It measures performance using standard metrics like CER, WER, and BLEU.\n",
       "* **Model Saving:** It saves trained model weights for later use in translation.\n",
       "\n",
       "**In short, you would use this script to build and train a machine translation system using a transformer architecture.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Explaining a script from a GitHub repository.\n",
    "\n",
    "file_contents = !curl https://github.com/hkproj/pytorch-transformer/blob/main/train.py\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
